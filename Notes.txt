ETCD is a distributed reliable key-value store that is simple, secure& fast
by default listen on port 2379
./etcdctl set key1 value1           # etcd command lines used to store and retrieve keys and values, it's version 2 command
./etcdctl --version
ETCDCTL_API=3 ./etcdctl version     # to make api version 3.3 rather than 2
./etcdctl put key1 value1           # to put value of key1
./etcdctl get key1                  # to get value of key1
---
#you must specify the ETCDCTL API version and path to certificate files
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key"
---
shceduler decide which pod go to which node but the kubelet creates the pods 
kubeadm doesn't automatically install the kubelet but you must always manually install the kubelet on your worker nodes. download the installer, extract and and run it as a service
Kube-Proxy is a network proxy that runs on each node in a Kubernetes cluster. It is responsible for maintaining network connectivity between services and pods. 
Kube-Proxy does this by translating service definitions into actionable networking rules.

kubectl run nginx --image nginx
kubectl get pods                    # to get running pods 
#pods with YAML
file name is pod-definition.yml
---
apiVersion: v1
kind: Pod                           # could be replicaset, deployment, service 
metadata:                           # data about the object
  name: myapp-pod
  labels:
      app: myapp                    # everything under metadata is written as dictionary not string 
      type: front-end               # you can name it tier not type
spec:
  containers:                       # is a list or arrary because we could have multible containers within them
    - name: nginx-containers
      image: nginx 
#   - name: busybox
#     image: busybox
---
kubectl create -f pod-definition.yml #to create the pod 
kubectl get pods
kubectl describe pod myapp-pod      #to get all info and details about your pod 
kubectl create -f pod-definition.yml = kubectl apply -f pod-definition.yml
kubectl delete pod <podName>
kubectl apply -f pod-definition.yml 
kubectl get pods -o wide            # to get wide view for get pods 
kubectl run redis --image=redis123 --dry-run=client -o yaml        # to make it wihout creating the yaml file 
----------------
---ReplicationController---
---
apiVersion: v1
kind: ReplicationController        
metadata:                           #for application controller                 
  name: myapp-rc
  labels:
      app: myapp                    
      type: front-end               
spec:
  template:
    metadata:                        #for pods 
      name: myapp-pod
      labels:
          app: myapp                    
          type: front-end               
    spec:
      containers:                       
        - name: nginx-containers
          image: nginx 

  replicas: 2
---
kubectl create -f rc-definition.yml   #replication controller created 
kubectl get ReplicationController     #to get desired, current, ready, and age of RC created
---------------------------------------------------------------------------------------------
---ReplicaSet---
---
apiVersion: apps/v1
kind: ReplicaSet        
metadata:                           #for replicaset                
  name: myapp-replicaset
  labels:
      app: myapp                    
      type: front-end               
spec:
  template:
    metadata:                        #for pods 
      name: myapp-pod
      labels:
          app: myapp                    
          type: front-end               
    spec:
      containers:                       
        - name: nginx-containers
          image: nginx 

  replicas: 
  selector:                          # determine how many pods are under control of replicaset and this is performed with the matchlabel 
    matchLabels:
      type: front-end 
------------------------------------------------------------------------
kubectl create -f replicaset-definition.yml 
kubectl get replicaset
#replicaset monitor the pods so if there's a pod that fails, it will run a new one
kubectl replace -f replicaset-definition.yml                #after updating the replicas in the file at anytime, run this command to apply 
kubectl scale --replicas=6 -f replicaset-definition.yml     #rather that updating in file to update replicas number 
kubectl scale --replicas=6 -f replicaset-definition.yml = kubectl scale --replicas=6 replicaset myapp-replicaset
kubectl delete replicaset myapp-replicaset
kubectl explain replicaset                                  #to see version and kind 
kubectl get rs 
kubectl edit rs <replicasetname>                            #navigate to the replica manifest file 
-------------------------------------------------------------------------------
---deployment---
---
apiVersion: apps/v1
kind: Deployment        
metadata:                                     
  name: myapp-deployment
  labels:
      app: myapp                    
      type: front-end               
spec:
  template:
    metadata:                        
      name: myapp-pod
      labels:
          app: myapp                    
          type: front-end               
    spec:
      containers:                       
        - name: nginx-container
          image: nginx 

  replicas: 3
  selector:              
    matchLabels:
      type: front-end 
---
kubectl create -f deployment-definition.yml 
kubectl get deployments
#the deployment automatically create a replicaset 
kubectl get all                                           # to see all created objects
--------------------------------------------------------------------------------------
---Service---
---
apiVersion: v1
kind: Service        
metadata:                                     
  name: myapp-service 
  labels:
      app: myapp                    
      type: front-end               
spec:
  type: NodePort                                #default type is ClusterIP
  ports:
   - targetPort: 80
     port: 80                                   #if you don't provide port,it's by default 80
     nodePort: 30008                            #if you don't provide nodePort,it's by default from 30000 to 32767
  selector:                                     #pull the labels from the pod definition file and put them under selector
     app: myapp
     type: front-end  
---
kubectl create -f service-definition.yml 
kubectl get services
#to set type to LoadBalancer, you need to integrate to a LoadBalancer service like ELB services from the cloud providers 
#Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. 
#Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.
#Namespaces are a way to divide cluster resources between multiple users (via resource quota).

mysql.connect("db-service")                         # to connect to a database in the same namespace
mysql.connect("db-service.dev.svc.cluster.local")   #to connect to a database in another namespace(servicename.namespace.svc.cluster.local)format
#cluster.local is the default domain name of the k8s cluster and svc is the subdomain (service )

kubectl get pods --namespace=<namespaceName>        #to get pods in a specific namespace and not the default one
kubectl create -f pod-definition.yml --namespace=dev
---------------------
---NameSpace---
---
apiVersion: v1
kind: Namespace
metadata:
    name: dev
---
kubectl create -f namespace-dev.yml
kubectl create namespace dev
kubectl config set-context $(kubectl config current-context) --namespace=dev      #to move permanently to this namespace 
kubectl get pods --all-namespaces
kubectl get pods -n=finance
kubectl run redis --image=redis -n=finance  #run pod named redis with image and namespace 
----------------------------------
#to limit resources of cluster within a namespace define a quota
---
apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-quota
    namespace: dev 
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi
---
kubectl get pods --all-namespaces = kubectl get pods -A 
kubectl get svc -n=<namespaceName>                        #get services in specific namespace
kubectl set image deployment nginx nginx=nginx:1.18 
kubectl expose deployment nginx --port 80
#--dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. 
#This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
#-o yaml: This will output the resource definition in YAML format on screen.


#Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 (This will automatically use the podâ€™s labels as selectors)
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

kubectl create service clusterip httpd --tcp=80:80
-----------------------------------------------------------------------------
#without a scheduler, the easiest way to shcedule a pod is to simply set the nodeName field to the name of the node in the pod spec file "parallel to containers:"
#if you want to add an existing pod to a node, create a binding object and send a post request to the pod binding API thus mimicking what 
the actual scheduler does. 
---
apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: node
  name: <nameoftargetnode>
  ---
  curl --header "content-type:application/json" --request POST --data '{"apiVersion":"v1", "kind": "Binding"...}
kubectl get pods -n kube-system                           #show the k8s system pods like etcd, scheduler..
kubectl replace --force -f nginx.yaml                     #rather that delete and create the pod again
kubectl get pods --watch 
kubectl get pods --selector <labelname>=<labelvalue>      #to select a pod with a label 
#You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.
#annotations are not used to identify and select objects. 
#The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. 
#It is possible to use labels as well as annotations in the metadata of the same object.
///
"metadata": {
  "annotations": {
    "key1" : "value1",
    "key2" : "value2"
  }
}
///
kubectl get all --selector env=prod,bu=finance,tier=frontend        #give a pod shared with all these selectors 
kubectl get pods --selector env=dev --no-headers | wc -l            #get number of pods with label env dev 
--------------------------------------
---Taints and Tolerations---
kubectl taint nodes <nodename> key=value:taint-effect               #to taint a node
#there are three main effects: NoSchedule PreferNoSchedule NoExecute
#https://www.hackertouch.com/what-is-the-difference-between-noexecute-noschedule-prefernoschedule.html  #to explain difference
#to add toleration to a pod
---
apiVersion: v1
kind: Pod
metadata:
  name: podName
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
---
kubectl taint nodes node1 
#if you want to restict the pods to certain nodes, that is another concept called node affinity 
#when k8s cluster initiated, a taint already put on the worker node not to get pods through scheduler
#to see this taint
kubectl describe node kubemaster | grep taint
kubectl describe nodes node01 | grep -i taints
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-    #untint the node with the tint value (node-role.kubernetes.io/control-plane:NoSchedule)
kubectl run podname -image=imagename --dry-run=client -o yaml > file.yaml             #create a pod content in file.yaml with dry run (without creating it)
-----------------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processing
    image: data-processor
  nodeSelector:
    size: large                                           #this will be a label on the node that we want to select
----------------
#to label a node
kubectl label node <nodename> <label-key>=<label-value>
------
---node affinity---
#the purpose of it is to ensure that the pod will select a particular node
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processing
    image: data-processor
  affinity:
   nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: size
          operator: In                                # the In operator make sure that this pod will be in the node with the key and value label
          values:
          - Large
-----------------
# you can make the labels like 
          - Large
          - Medium
# to make sure it could be in a node with this label or this label
# we can put the operator to "NotIn" and that's means this pod will not be at the node with the key value labels 
#another label "Exists" check if the label exists on the nodes 
-------------------------------------
---Node Affinity Types---
availabel:                                                #any change in node affinity after running will not affect the pod
requiredDuringSchedulingIgnoredDuringExecution            #if the matching pod does not exist, the pod will not be scheduled 
preferredDuringSchedulingIgnoredDuringExecution           #if the matching pod does not exist, the scheduler will simply ignore node affinity rules

planned:                                                  #during execution means a change while the pod is running and this change affect node affinity
requiredDuringSchedulingRequiredDuringExecution           #required to affect the node after the running phase is exist 
preferredDuringSchedulingRequiredDuringExecution
-------------------------------------------
#taints and tolerations make sure that other pods will not be in the node that tainted
#but node affinity doesn't mean that there's no chance that other pods may end up on the node
#after we use taints and tolerations to prevent other pods from being placed on our nodes
#we use affinity to prevent our pods from being placed on their nodes
#https://medium.com/saas-infra/taints-and-tolerations-node-affinity-and-node-selector-explained-f329653c2bc6#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjZmOTc3N2E2ODU5MDc3OThlZjc5NDA2MmMwMGI2NWQ2NmMyNDBiMWIiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDAxNzA5OTMxMTAwMjM3MTkxODciLCJlbWFpbCI6Im1vbW9tczIwMTBAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTcwOTYyMzcyMSwibmFtZSI6Ik1vaGFtZWQgU2FsYWgiLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jSUxuSVl5YU9NSnktaXE3OTZlY1pWQVNIMWpLTXEwX1dIdktmeTNJZjRnVlJJPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6Ik1vaGFtZWQiLCJmYW1pbHlfbmFtZSI6IlNhbGFoIiwibG9jYWxlIjoiZW4iLCJpYXQiOjE3MDk2MjQwMjEsImV4cCI6MTcwOTYyNzYyMSwianRpIjoiMWMyMjc1ODM4MDIzYTZlYThmNGE5NWE5N2I3N2Y4NzdjODgwYzk4NCJ9.Ji4NWJZuHipep3pPQC-3KoAIUoNosTTvgF5sYxw8raqYj84XAhBSxLLPXH9uNplCQlRZG1U_XAvtc6rrKKEitKA0ZNKbD8IyvidUWfLRQvISkfme1qnvvNdsmRxQ4N1PEPowh8VcqIu1Cy-4VLCCIo_q14x9ZskwvFyNX4bVotcTGhZ-KeqERqRavXegWxP_lvmqjDdcD4HCLz04nY0Kp7DQ9DXwmoMjoYcdSgUocuvBS1IM9GPfvXM-Jwdfe5KRvp_ojwspfS47WY-DdPIVxKOQSbfy5uB-IGEbl-m2BvITT_0o33ZoMTzOewUIv6EDnKqO-cUkRauhycYLo3eUQg
--------------------------------------------
---resource requests---
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    name:webapp
spec:
  containers:
  - name: webapp
    image: nginx
    ports:
      - containerport:8080
    resources:                                        # you can specify/request minimum resources for your pod here 
      requests:
        memory: "4Gi"
        cpu: 2
      limits:                                         #you can specify the limits of your resources here 
        memory: "6Gi"
        cpu: 4
-----------------------------
#the scheduler will looks at a node that has that amount of resources
#if a pod does not have a request field and other pods consume all resources, this pod could starve of lacking of resources
#we cannot throttle memory like CPU, so the only way to retrieve memory from a pod is to kill the pod to free up some memory
-----------------------------
---limit range---
#limit-range-cpu.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:                          #limit
      cpu: 500m
    defaultRequest:                   #request 
      cpu: 500m
    max:                              #limit
      cpu: "1"
    min:                              #request 
      cpu: 100m
    type: container 
-----------------------------
#limit-range-memory.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - default:                          #limit
      memory: 1Gi 
    defaultRequest:                   #request 
      memory: 1Gi 
    max:                              #limit
      memory: 1Gi
    min:                              #request 
      memory: 500Mi
    type: container 
-----------------------------
---resource quotas---
#resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name:my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi
---
#A DaemonSet ensures that all (or some) Nodes run a copy of a Pod.
#As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, 
#those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.
---daemonSet Definition---
#daemon-set-definition.yaml > same as replicaset file
---
apiVersion: apps/v1
kind: daemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
      - name: monitoring-agent
        image: monitoring-agent
------------------------------
kubectl create -f daemon-set-definition.yaml
kubectl get daemonsets 
kubectl describe daemonsets <daemonsetname>
kubectl get daemonsets --all-namespaces = kubectl get daemonsets -A 
#as mentioned before, in pod file, set the nodeName property to determine the exact node that the pod will be in (until version 1.12)
#from v1.12 - use nodeAffinity and default scheduler
kubectl describe daemonset <daemonsetName> --namespace=<namespaceName> 
#imperative example:
kubectl create deployment <deploymentname> -n <namespaceName> --image=<imagename> --dryrun=client -o yaml > file.yaml 
----------------------------------
#Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. 
#Unlike Pods that are managed by the control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).
#Static Pods are always bound to one Kubelet on a specific node.
---
# Choose a directory, say /etc/kubernetes/manifests and place a web server Pod definition there, for example /etc/kubernetes/manifests/static-web.yaml:
# Run this command on the node where kubelet is running
mkdir -p /etc/kubernetes/manifests/
cat <<EOF >/etc/kubernetes/manifests/static-web.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-web
  labels:
    role: myrole
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
EOF
---
#if you want to get static pods in all name spaces 
#Run the command kubectl get pods --all-namespaces and look for those with -controlplane appended in the name
#to get which nodes are the static pods created:
#kubectl get pods -A -o wide
#how to know the path for the static pod 
ps -aux | grep kubelet #identify the config file > --config=/var/lib/kubelet/config.yaml
ps -aux | grep /usr/bin/kubelet
grep -i staticpod /var/lib/kubelet/config.yaml
-------
#creating static pod
kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-busybox
  name: static-busybox
spec:
  containers:
  - command:
    - sleep
    - "1000"
    image: busybox
    name: static-busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
---
---multiple schedulers---
#to create another scheduler, you have to give it a file with another name than the default-scheduler name 
---
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: <name>
---
#another way to deploy another scheduler as a pod
---
apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --config=/etc/kubernetes/my-scheduler-config.yaml

    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    name: kubescheduler
---
we have to add leaderelection if we have HA and more than one master node, and want to set an active node from a specific shceduler
---
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: <name>
leaderElection:
  leaderElect: true                             #if exist,only the leader instance will actively schedule pods. If the leader instance fails, another instance will be elected as the new leader to continue scheduling pods.
  resourceNamespace: kube-system
  resourceName: lock-object-my-scheduler
---
#to deploy an additional scheduler
#https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/

kubectl get pods --namespace=kube-system  #to see system pods like schedulers that you have 
#to use the new scheduler, while creating the pod specify the scheduler name 
spec:
  containers:
  - image: nginx
    name: nginx
  schedulerName: my-custom-scheduler
---
kubectl create -f pod.yaml 
kubectl get events -o wide                                      #to know which scheduler schedule which pod
kubectl logs my-custom-scheduler --name-space=kube-system       #to get logs
#you can determine priority of the pod 
spec:
  priorityClassName: high-priority                              #it will be shceduled first
---
---configure scheduler profiles---
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: <name>
  plugins:
    score:
      disabled:
       - name: TaintToleration
      enabled:
       - name: MyCustomPluginA
       - name: MyCustomPluginB
--------------------------------------
--------------------------------------
---monitoring and logging---
#to enable metric-server in minikube cluster 
minikube addons enable metrics-server
#for other environments
git clone https://github.com/kubernetes-sigs/metrics-server.git
kubectl create -f deploy/1.8+/                                    #deploy pods to enable metrics server to poll for perfomance metrics from the nodes in cluster 

kubectl top node                                                  #cpu and memory consumption for nodes 
kubectl top pod 
-------
---application logs---
kubectl logs -f <podname>
kubectl logs -f <podname> <containername>                         #to get a specific container's log inside the pod 
----------------------------------------
kubectl rollout status deployment/<nameofdeployment>              #to see the status of the rollout
kubectl rollout history deployment/<nameofdeployment>             #show the revisions and history of the rollout
#rolling update is the default deployment strategy
#just run the kubectl apply deployment command and the upgrade will take place 
#but you can use kubectl set image command to update the image of your app 
kubectl rollout undo deployment/<deploymentname                   #to undo the rollout of the deployment (rollback)
---
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
---
  strategy:
    type: Recreate
----
---create a pod from docker image with sleep command---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleep-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      args: ["10"]                                          # it's like entrypoint: sleep CMD: 10 and like in linux sleep 10 command 
---
#the args section override the CMD part in the docker file 
#if you want to override the entrypoint field from the pod file 
containers:
  - name: 
    command: ["sleep 2.0"]   #to override the entrypoint field 
    args: ["10"]
----
---environment variable---
docker run -e APP_COLOR=pink simple-webapp-color
#equal in k8s
spec:
  env:
    - name: APP_COLOR
      value: pink                       #this way named plain key value 
---
env:
  - name: APP_COLOR
    valueFrom: 
        configMapKeyRef:                #config map way
---
env:
  - name: APP_COLOR
    valueFrom:
        secretKeyRef:                  #secret way 
---
---configmap--
#configmapfile
APP_COLOR: blue
APP_MODE: prod
---
kubectl create -f ...       #declarative way 
kubectl create configmap <config-name> --from-literal=<key>=<value>   #imperative way 
#example kubectl create configmap \ app-config --from-literal=APP_COLOR=blue \ --from-literal=APP_MOD=prod 
#another way
kubectl create configmap <configname> --from-file=<path-to-file>
kubectl ceate configmap \ app-config --from-file=app.config.properties

#declarative
config-map.yaml
apiVersion: v1
kind: configMap
metadata:
  name: app-config 
data:
  APP_COLOR:blue
  APP_MOD:prod
---
kubectl get configmaps  #also describe command 
#how to link it with pod 
---
spec:
  envFrom:
    - configMapRef:
          name: app-config
---
spec:
  valueFrom
    configMapKeyRef:
      name: app-config
      key: APP_COLOR         #another way 
---
kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
#from label
---
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
---
---secrets---
kubectl create secret generic <secretname>   #imperative
kubectl create secret generic app-secret --from-literal=DB_Host=mysql --from-literal=DB_User=root --from-literal=DB_Password=paswrd 
kubectl create secret generic <secretname> --from-file=<pathtofile>
kubectl create secret generic <secretname>
---
#secret-data.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: mysql 
  DB_User: root
  DB_Password: paswrd
---
#to hash the values above for more security 
echo -n 'mysql' | base64                    #to convert mysql into hashing value then put the hashed values in the file 
kubectl get secrets 
kubectl get secret <secretname> -o yaml 
echo -n 'bXlzcWw=' | base64 > mysql          #to decode 

#now configure it with the pod 
containers:
  envFrom:
    - secretRef:
          name: app-secret
---
#another way
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
        name: app-secret
        key: DB_Password
---
#inject all secrets as file in the volume
volumes:
- name: app-secret-volume
  secret: 
    secretName: app-secret
---
ls /opt/app-secret-volumes              > DB_Host DB_Password DB_User
cat /opt/app-secret-volumes/DB_Password > paswrd
----
apt-get install etcd-client                                           #to install etcd and use etcdctl command
ps -aux | grep kube-api | grep "encryption-provider-config"           #to check if encryption at rest enabled or not
#to enable encryption
head -c 32 /dev/urandom | base64
vim enc.yaml
---
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: <result from the head command>
      - identity: {}
---
mkdir /etc/kubernetes/enc
mv enc.yaml /etc/kubernetes/enc/
vim /etc/kubernetes/manifests/kube-apiserver.yaml 
containers:
  - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml 
#also add volume mounts 
#add specification for location of local directory
